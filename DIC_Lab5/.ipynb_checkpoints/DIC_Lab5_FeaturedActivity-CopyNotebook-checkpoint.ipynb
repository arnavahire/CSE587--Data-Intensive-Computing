{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/arnav/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7b3359eddb62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/arnav/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 "
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#textFile = sc.textFile(\"/home/arnav/Desktop/CSE_587_DIC/latin/ambrose.apologia_david_altera.tess\")\n",
    "textFile = sc.textFile(\"myFolder\")\n",
    "textFile.filter(lambda x: x is not None).filter(lambda x: x != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "mydict= dict()\n",
    "with open(\"new_lemmatizer.csv\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        temp = \"\"\n",
    "        k= row[0]\n",
    "        length = len(row)\n",
    "        for i in range(1,length):\n",
    "            if row[i] != \"\":\n",
    "                temp += row[i]+\" \"\n",
    "        mydict[k] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3777.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/arnav/Desktop/CSE_587_DIC/DIC_Lab5/temporary already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1177)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-9c533ab10960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyMapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3777.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/arnav/Desktop/CSE_587_DIC/DIC_Lab5/temporary already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1177)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "def myMapper(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "            \n",
    "        for j in range(i+1,len(tokens)):\n",
    "            if(i==j):\n",
    "                continue\n",
    "            normalizedNeighbour=tokens[j].replace('j','i')\n",
    "            normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "            normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "            normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "            finalKeyVal=normalizedWord+\" \"+normalizedNeighbour\n",
    "            \n",
    "            keyValList.append((finalKeyVal,location))\n",
    "            \n",
    "    return keyValList\n",
    "\n",
    "output=textFile.flatMap(myMapper)\n",
    "output.saveAsTextFile(\"temporary\")\n",
    "\n",
    "output1=output.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output1.saveAsTextFile(\"temporary1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3892.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/arnav/Desktop/CSE_587_DIC/DIC_Lab5/temporary2 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1177)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-7f75880a836d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyMapperForThreeWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0moutput2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporary2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0moutput3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0moutput3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporary3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3892.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/arnav/Desktop/CSE_587_DIC/DIC_Lab5/temporary2 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1177)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "def myMapperForThreeWords(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    \n",
    "    for i in range(0,len(tokens)-2):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "            \n",
    "        for j in range(i+1,len(tokens)-1):\n",
    "            if(i==j):\n",
    "                continue\n",
    "            normalizedNeighbour=tokens[j].replace('j','i')\n",
    "            normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "            normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "            normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "            for k in range(j+1,len(tokens)):\n",
    "                normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2 # (key-value, location)\n",
    "                keyValList.append((finalKeyVal,location))\n",
    "            \n",
    "    return keyValList\n",
    "\n",
    "output2=textFile.flatMap(myMapperForThreeWords)\n",
    "output2.saveAsTextFile(\"temporary2\")\n",
    "output3=output2.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output3.saveAsTextFile(\"temporary3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 13.3856709003 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapper(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):\n",
    "            str=mydict.get(normalizedWord).split(\" \")  # we will get an array of lemmas\n",
    "            for x in range(0,len(str)):\n",
    "                for j in range(i+1,len(tokens)):\n",
    "                    if(i==j):\n",
    "                        continue\n",
    "                    normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                    finalKeyVal=str[x]+\" \"+normalizedNeighbour\n",
    "                    keyValList.append((finalKeyVal,location))\n",
    "                                        \n",
    "        else:\n",
    "            for j in range(i+1,len(tokens)):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour\n",
    "                keyValList.append((finalKeyVal,location))\n",
    "                        \n",
    "    return keyValList\n",
    "\n",
    "output=textFile.flatMap(myMapper)\n",
    "output.saveAsTextFile(\"temporaryf1\")\n",
    "\n",
    "output1=output.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output1.saveAsTextFile(\"temporaryf2\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 14.0326988697 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapper(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):        # dictionary contains word\n",
    "            wordLemma=mydict.get(normalizedWord).split(\" \")  # we will get an array of lemmas for word\n",
    "            for x in range(0,len(wordLemma)):          # for each lemma of word\n",
    "                    for j in range(i+1,len(tokens)):\n",
    "                        if(i==j):\n",
    "                            continue\n",
    "                        normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                        if(mydict.has_key(normalizedNeighbour)):     # dictionary contains neighbour\n",
    "                            neighbour1Lemma=mydict.get(normalizedNeighbour).split(\" \")\n",
    "                            for y in range(0,len(neighbour1Lemma)):     # for each lemma of neighbour, append (lemma(normalizedWord),lemma(normalizedNeighbour):location)\n",
    "                                finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]\n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                        else:\n",
    "                            finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour # if dictionary doesn't contain neighbour, append (lemma(normalizedWord),normalizedNeighbour:location)\n",
    "                            keyValList.append((finalKeyVal,location))\n",
    "                                        \n",
    "        else:                                 # if no lemma for word and neighbour present, then append (normalizedWord,normalizedNeighbour:location)\n",
    "            for j in range(i+1,len(tokens)):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour\n",
    "                keyValList.append((finalKeyVal,location))\n",
    "                        \n",
    "    return keyValList\n",
    "\n",
    "output=textFile.flatMap(myMapper)\n",
    "output.saveAsTextFile(\"temporaryf1\")\n",
    "\n",
    "output1=output.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output1.saveAsTextFile(\"temporaryf2\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.6182248592 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapperForThreeWords(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    \n",
    "    for i in range(0,len(tokens)-2):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):        # dictionary contains word\n",
    "            wordLemma=mydict.get(normalizedWord).split(\" \")  # we will get an array of lemmas for word\n",
    "            for x in range(0,len(wordLemma)):\n",
    "                for j in range(i+1,len(tokens)-1):\n",
    "                    if(i==j):\n",
    "                        continue\n",
    "                    normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                    if(mydict.has_key(normalizedNeighbour)):     # dictionary contains neighbour\n",
    "                        neighbour1Lemma=mydict.get(normalizedNeighbour).split(\" \")\n",
    "                        for y in range(0,len(neighbour1Lemma)):\n",
    "                            for k in range(j+1,len(tokens)):\n",
    "                                normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                                if(mydict.has_key(normalizedNeighbour2)):  # lemmas for neighbour2\n",
    "                                    neighbour2Lemma=mydict.get(normalizedNeighbour2).split(\" \")\n",
    "                                    for z in range(0,len(neighbour2Lemma)):\n",
    "                                        finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]+\" \"+neighbour2Lemma[z]    # append (lemma(normalizedWord),lemma(normalizedNeighbour1),lemma(normalizedNeighbour2):location)                   \n",
    "                                        keyValList.append((finalKeyVal,location))\n",
    "                                else:\n",
    "                                    finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]+\" \"+normalizedNeighbour2      # append (lemma(normalizedWord),lemma(normalizedNeighbour1),normalizedNeighbour2:location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                    else:  # neighbour1 is not present in dictionary\n",
    "                        for k in range(j+1,len(tokens)):\n",
    "                            normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                            if(mydict.has_key(normalizedNeighbour2)):\n",
    "                                neighbour2Lemma=mydict.get(normalizedNeighbour2).split(\" \")\n",
    "                                for z in range(0,len(neighbour2Lemma)):\n",
    "                                    finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour+\" \"+neighbour2Lemma[z]  # append (lemma(normalizedWord),normalizedNeighbour1,lemma(normalizedNeighbour2):location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                            else:\n",
    "                                finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2    # append (lemma(normalizedWord),normalizedNeighbour1,normalizedNeighbour2:location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "        else:\n",
    "            for j in range(i+1,len(tokens)-1):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                if(mydict.has_key(normalizedNeighbour)):\n",
    "                    neighbour1Lemma=mydict.get(normalizedNeighbour).split(\" \")\n",
    "                    for y in range(0,len(neighbour1Lemma)):\n",
    "                        for k in range(j+1,len(tokens)):\n",
    "                            normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                            if(mydict.has_key(normalizedNeighbour2)):\n",
    "                                neighbour2Lemma=mydict.get(normalizedNeighbour2).split(\" \")\n",
    "                                for z in range(0,len(neighbour2Lemma)):\n",
    "                                    finalKeyVal=normalizedWord+\" \"+neighbour1Lemma[y]+\" \"+neighbour2Lemma[z]  # append (normalizedWord,lemma(normalizedNeighbour1),lemma(normalizedNeighbour2):location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                            else:\n",
    "                                finalKeyVal=normalizedWord+\" \"+neighbour1Lemma[y]+\" \"+normalizedNeighbour2    # append (normalizedWord,lemma(normalizedNeighbour1),normalizedNeighbour2:location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                else:\n",
    "                    for k in range(j+1,len(tokens)):\n",
    "                        normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                        if(mydict.has_key(normalizedNeighbour2)):\n",
    "                            neighbour2Lemma=mydict.get(normalizedNeighbour2).split(\" \")\n",
    "                            for z in range(0,len(neighbour2Lemma)):\n",
    "                                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+neighbour2Lemma[z]    # append (normalizedWord,normalizedNeighbour1,lemma(normalizedNeighbour2):location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                        else:\n",
    "                            finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2        # append (normalizedWord,normalizedNeighbour1,normalizedNeighbour2:location) \n",
    "                            keyValList.append((finalKeyVal,location))       \n",
    "            \n",
    "    return keyValList\n",
    "\n",
    "output2=textFile.flatMap(myMapperForThreeWords)\n",
    "output2.saveAsTextFile(\"temporary2\")\n",
    "output3=output2.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output3.saveAsTextFile(\"temporary3\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 15.3954207897 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapper(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):        # dictionary contains word\n",
    "            wordLemma=mydict.get(normalizedWord).strip()  # remove all extra spaces in strings\n",
    "            wordLemma=wordLemma.split(\" \")                  # we will get an array of lemmas for word\n",
    "            for x in range(0,len(wordLemma)):          # for each lemma of word\n",
    "                    for j in range(i+1,len(tokens)):\n",
    "                        if(i==j):\n",
    "                            continue\n",
    "                        normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                        if(mydict.has_key(normalizedNeighbour)):     # dictionary contains neighbour\n",
    "                            neighbour1Lemma=mydict.get(normalizedNeighbour).strip()   # remove extra spaces\n",
    "                            neighbour1Lemma=neighbour1Lemma.split(\" \")\n",
    "                            for y in range(0,len(neighbour1Lemma)):     # for each lemma of neighbour, append (lemma(normalizedWord),lemma(normalizedNeighbour):location)\n",
    "                                finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]\n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                        else:\n",
    "                            finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour # if dictionary doesn't contain neighbour, append (lemma(normalizedWord),normalizedNeighbour:location)\n",
    "                            keyValList.append((finalKeyVal,location))\n",
    "                                        \n",
    "        else:                                 # if no lemma for word and neighbour present, then append (normalizedWord,normalizedNeighbour:location)\n",
    "            for j in range(i+1,len(tokens)):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour\n",
    "                keyValList.append((finalKeyVal,location))\n",
    "                        \n",
    "    return keyValList\n",
    "\n",
    "output=textFile.flatMap(myMapper)\n",
    "output.saveAsTextFile(\"tp1\")\n",
    "\n",
    "output1=output.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output1.saveAsTextFile(\"tp2\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.9582121372 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapperForThreeWords(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    \n",
    "    for i in range(0,len(tokens)-2):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):        # dictionary contains word\n",
    "            wordLemma=mydict.get(normalizedWord).strip() # remove extra spaces\n",
    "            wordLemma=wordLemma.split(\" \") # we will get an array of lemmas for word\n",
    "            for x in range(0,len(wordLemma)):\n",
    "                for j in range(i+1,len(tokens)-1):\n",
    "                    if(i==j):\n",
    "                        continue\n",
    "                    normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                    if(mydict.has_key(normalizedNeighbour)):     # dictionary contains neighbour\n",
    "                        neighbour1Lemma=mydict.get(normalizedNeighbour).strip()\n",
    "                        neighbour1Lemma=neighbour1Lemma.split(\" \")\n",
    "                        for y in range(0,len(neighbour1Lemma)):\n",
    "                            for k in range(j+1,len(tokens)):\n",
    "                                normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                                if(mydict.has_key(normalizedNeighbour2)):  # lemmas for neighbour2\n",
    "                                    neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                                    neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                                    for z in range(0,len(neighbour2Lemma)):\n",
    "                                        finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]+\" \"+neighbour2Lemma[z]    # append (lemma(normalizedWord),lemma(normalizedNeighbour1),lemma(normalizedNeighbour2):location)                   \n",
    "                                        keyValList.append((finalKeyVal,location))\n",
    "                                else:\n",
    "                                    finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]+\" \"+normalizedNeighbour2      # append (lemma(normalizedWord),lemma(normalizedNeighbour1),normalizedNeighbour2:location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                    else:  # neighbour1 is not present in dictionary\n",
    "                        for k in range(j+1,len(tokens)):\n",
    "                            normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                            if(mydict.has_key(normalizedNeighbour2)):\n",
    "                                neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                                neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                                for z in range(0,len(neighbour2Lemma)):\n",
    "                                    finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour+\" \"+neighbour2Lemma[z]  # append (lemma(normalizedWord),normalizedNeighbour1,lemma(normalizedNeighbour2):location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                            else:\n",
    "                                finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2    # append (lemma(normalizedWord),normalizedNeighbour1,normalizedNeighbour2:location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "        else:\n",
    "            for j in range(i+1,len(tokens)-1):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                if(mydict.has_key(normalizedNeighbour)):\n",
    "                    neighbour1Lemma=mydict.get(normalizedNeighbour).strip()\n",
    "                    neighbour1Lemma=neighbour1Lemma.split(\" \")\n",
    "                    for y in range(0,len(neighbour1Lemma)):\n",
    "                        for k in range(j+1,len(tokens)):\n",
    "                            normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                            if(mydict.has_key(normalizedNeighbour2)):\n",
    "                                neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                                neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                                for z in range(0,len(neighbour2Lemma)):\n",
    "                                    finalKeyVal=normalizedWord+\" \"+neighbour1Lemma[y]+\" \"+neighbour2Lemma[z]  # append (normalizedWord,lemma(normalizedNeighbour1),lemma(normalizedNeighbour2):location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                            else:\n",
    "                                finalKeyVal=normalizedWord+\" \"+neighbour1Lemma[y]+\" \"+normalizedNeighbour2    # append (normalizedWord,lemma(normalizedNeighbour1),normalizedNeighbour2:location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                else:\n",
    "                    for k in range(j+1,len(tokens)):\n",
    "                        normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                        if(mydict.has_key(normalizedNeighbour2)):\n",
    "                            neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                            neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                            for z in range(0,len(neighbour2Lemma)):\n",
    "                                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+neighbour2Lemma[z]    # append (normalizedWord,normalizedNeighbour1,lemma(normalizedNeighbour2):location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                        else:\n",
    "                            finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2        # append (normalizedWord,normalizedNeighbour1,normalizedNeighbour2:location) \n",
    "                            keyValList.append((finalKeyVal,location))       \n",
    "            \n",
    "    return keyValList\n",
    "\n",
    "output2=textFile.flatMap(myMapperForThreeWords)\n",
    "output2.saveAsTextFile(\"myoutput1\")\n",
    "output3=output2.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output3.saveAsTextFile(\"myoutput2\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
