{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featured Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 'Python' as the programming language for implementing our word co-occurrence problem in Apache spark. For that we need the 'pyspark' library which enables us to usepython for Spark programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/arnav/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7b3359eddb62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/arnav/Desktop/CSE_587_DIC/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/arnav/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 "
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be taking inputs from a folder called 'Input'. So whatever files are present in this folder will be used as the input to our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"Input\")\n",
    "textFile=textFile.filter(lambda x: x is not u'')\n",
    "# textFile is our RDD and we will extract all the required information from this RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Dictionary for Lemmas' look up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary which is an equivalent of hash-map in Python. We will use this dictionary to perform our lookup operations for Lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "mydict= dict()\n",
    "with open(\"new_lemmatizer.csv\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        temp = \"\"\n",
    "        k= row[0]\n",
    "        length = len(row)\n",
    "        for i in range(1,length):\n",
    "            if row[i] != \"\":\n",
    "                temp += row[i]+\" \"\n",
    "        mydict[k] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word-co-occurence Bi-grams (n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentioned comments in the code enable us to understand the overall flow of our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.95885300636 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapper(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):        # dictionary contains word\n",
    "            wordLemma=mydict.get(normalizedWord).strip()  # remove all extra spaces in strings\n",
    "            wordLemma=wordLemma.split(\" \")                  # we will get an array of lemmas for word\n",
    "            for x in range(0,len(wordLemma)):          # for each lemma of word\n",
    "                    for j in range(i+1,len(tokens)):\n",
    "                        if(i==j):\n",
    "                            continue\n",
    "                        normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                        normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                        if(mydict.has_key(normalizedNeighbour)):     # dictionary contains neighbour\n",
    "                            neighbour1Lemma=mydict.get(normalizedNeighbour).strip()   # remove extra spaces\n",
    "                            neighbour1Lemma=neighbour1Lemma.split(\" \")\n",
    "                            for y in range(0,len(neighbour1Lemma)):     # for each lemma of neighbour, append (lemma(normalizedWord),lemma(normalizedNeighbour):location)\n",
    "                                finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]\n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                        else:\n",
    "                            finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour # if dictionary doesn't contain neighbour, append (lemma(normalizedWord),normalizedNeighbour:location)\n",
    "                            keyValList.append((finalKeyVal,location))\n",
    "                                        \n",
    "        else:                                 # if no lemma for word and neighbour present, then append (normalizedWord,normalizedNeighbour:location)\n",
    "            for j in range(i+1,len(tokens)):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour\n",
    "                keyValList.append((finalKeyVal,location))  # we create a tuple (key,val) and append this in our list\n",
    "                        \n",
    "    return keyValList  # return final list of all key-values\n",
    "\n",
    "output=textFile.flatMap(myMapper)          # Mapper method called\n",
    "output.saveAsTextFile(\"BiGramsMapperOutput\")\n",
    "\n",
    "output1=output.reduceByKey(lambda a,b:a+\",\"+b)  # Reducer method called\n",
    "output1.saveAsTextFile(\"BiGramsReducerOutput\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word-co-occurence Tri-grams (n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentioned comments in the code enable us to understand the overall flow of our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 24.6293430328 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def myMapperForThreeWords(line):\n",
    "    splitLocationText=line.split(\">\")       # Split (<location> Text) as (<location || Text) \n",
    "    location=splitLocationText[0]+\">\"       # Now we get location as <location>\n",
    "    finalLine=splitLocationText[1].strip()  # Trimming the line\n",
    "    tokens=finalLine.split(\" \")             # 1 token= 1 word in line\n",
    "    keyValList=[]                           # To store the keyValues and locations\n",
    "    \n",
    "    for i in range(0,len(tokens)-2):\n",
    "        normalizedWord=tokens[i].replace('j','i')\n",
    "        normalizedWord=normalizedWord.replace('J','I')\n",
    "        normalizedWord=normalizedWord.replace('v','u')\n",
    "        normalizedWord=normalizedWord.replace('V','U') # Normalize words by substituting 'j' by 'i' and 'v' by 'u'\n",
    "        if(mydict.has_key(normalizedWord)):        # dictionary contains word\n",
    "            wordLemma=mydict.get(normalizedWord).strip() # remove extra spaces\n",
    "            wordLemma=wordLemma.split(\" \") # we will get an array of lemmas for word\n",
    "            for x in range(0,len(wordLemma)):\n",
    "                for j in range(i+1,len(tokens)-1):\n",
    "                    if(i==j):\n",
    "                        continue\n",
    "                    normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                    normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                    if(mydict.has_key(normalizedNeighbour)):     # dictionary contains neighbour\n",
    "                        neighbour1Lemma=mydict.get(normalizedNeighbour).strip()\n",
    "                        neighbour1Lemma=neighbour1Lemma.split(\" \")\n",
    "                        for y in range(0,len(neighbour1Lemma)):\n",
    "                            for k in range(j+1,len(tokens)):\n",
    "                                normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                                normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                                if(mydict.has_key(normalizedNeighbour2)):  # lemmas for neighbour2\n",
    "                                    neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                                    neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                                    for z in range(0,len(neighbour2Lemma)):\n",
    "                                        finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]+\" \"+neighbour2Lemma[z]    # append (lemma(normalizedWord),lemma(normalizedNeighbour1),lemma(normalizedNeighbour2):location)                   \n",
    "                                        keyValList.append((finalKeyVal,location))\n",
    "                                else:\n",
    "                                    finalKeyVal=wordLemma[x]+\" \"+neighbour1Lemma[y]+\" \"+normalizedNeighbour2      # append (lemma(normalizedWord),lemma(normalizedNeighbour1),normalizedNeighbour2:location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                    else:  # neighbour1 is not present in dictionary\n",
    "                        for k in range(j+1,len(tokens)):\n",
    "                            normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                            if(mydict.has_key(normalizedNeighbour2)):\n",
    "                                neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                                neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                                for z in range(0,len(neighbour2Lemma)):\n",
    "                                    finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour+\" \"+neighbour2Lemma[z]  # append (lemma(normalizedWord),normalizedNeighbour1,lemma(normalizedNeighbour2):location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                            else:\n",
    "                                finalKeyVal=wordLemma[x]+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2    # append (lemma(normalizedWord),normalizedNeighbour1,normalizedNeighbour2:location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "        else:\n",
    "            for j in range(i+1,len(tokens)-1):\n",
    "                if(i==j):\n",
    "                    continue\n",
    "                normalizedNeighbour=tokens[j].replace('j','i')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('J','I')\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('v','u') # Normalize neighbour\n",
    "                normalizedNeighbour=normalizedNeighbour.replace('V','U')\n",
    "                if(mydict.has_key(normalizedNeighbour)):\n",
    "                    neighbour1Lemma=mydict.get(normalizedNeighbour).strip()\n",
    "                    neighbour1Lemma=neighbour1Lemma.split(\" \")\n",
    "                    for y in range(0,len(neighbour1Lemma)):\n",
    "                        for k in range(j+1,len(tokens)):\n",
    "                            normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                            normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                            if(mydict.has_key(normalizedNeighbour2)):\n",
    "                                neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                                neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                                for z in range(0,len(neighbour2Lemma)):\n",
    "                                    finalKeyVal=normalizedWord+\" \"+neighbour1Lemma[y]+\" \"+neighbour2Lemma[z]  # append (normalizedWord,lemma(normalizedNeighbour1),lemma(normalizedNeighbour2):location) \n",
    "                                    keyValList.append((finalKeyVal,location))\n",
    "                            else:\n",
    "                                finalKeyVal=normalizedWord+\" \"+neighbour1Lemma[y]+\" \"+normalizedNeighbour2    # append (normalizedWord,lemma(normalizedNeighbour1),normalizedNeighbour2:location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                else:\n",
    "                    for k in range(j+1,len(tokens)):\n",
    "                        normalizedNeighbour2=tokens[k].replace('j','i')\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('J','I')\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('v','u') # Normalize neighbour2\n",
    "                        normalizedNeighbour2=normalizedNeighbour2.replace('V','U')\n",
    "                        if(mydict.has_key(normalizedNeighbour2)):\n",
    "                            neighbour2Lemma=mydict.get(normalizedNeighbour2).strip()\n",
    "                            neighbour2Lemma=neighbour2Lemma.split(\" \")\n",
    "                            for z in range(0,len(neighbour2Lemma)):\n",
    "                                finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+neighbour2Lemma[z]    # append (normalizedWord,normalizedNeighbour1,lemma(normalizedNeighbour2):location) \n",
    "                                keyValList.append((finalKeyVal,location))\n",
    "                        else:\n",
    "                            finalKeyVal=normalizedWord+\" \"+normalizedNeighbour+\" \"+normalizedNeighbour2        # append (normalizedWord,normalizedNeighbour1,normalizedNeighbour2:location) \n",
    "                            keyValList.append((finalKeyVal,location))       \n",
    "            \n",
    "    return keyValList\n",
    "\n",
    "output2=textFile.flatMap(myMapperForThreeWords)\n",
    "output2.saveAsTextFile(\"TriGramsMapperOutput\")\n",
    "output3=output2.reduceByKey(lambda a,b:a+\",\"+b)\n",
    "output3.saveAsTextFile(\"TriGramsReducerOutput\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
